---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---

<p style="text-align:center;"><img width="35%" src="logo.png" alt="Logo" loading="lazy"></p>

[[Speakers](#speakers) · [Recordings](#recordings) · [References](#references)]

A two-day online workshop on compositionality and artificial intelligence organized by [Gary Marcus](http://garymarcus.com) and [Raphaël Millière](https://raphaelmilliere.com).



## Speakers

- [Stephanie Chan](https://scholar.google.com/citations?user=bXOt49QAAAAJ&hl=en) (DeepMind)
- [Allyson Ettinger](https://linguistics.uchicago.edu/allyson-ettinger) (University of Chicago)
- [Dieuwke Hupkes](https://dieuwkehupkes.nl/) (European Laboratory for Learning and Intelligent Systems / Meta AI)
- [Paul Smolensky](https://cogsci.jhu.edu/directory/paul-smolensky/) (Johns Hopkins University/Microsoft Research Redmond)
- [Brenden Lake](https://cims.nyu.edu/~brenden/) (New York University / Meta AI)
- [Tal Linzen](https://tallinzen.net/) (New York University / Google AI)
- [Gary Marcus](http://garymarcus.com) (New York University, Emeritus)
- [Raphaël Millière](https://raphaelmilliere.com) (Columbia University)
- [Ellie Pavlick](https://cs.brown.edu/people/epavlick/) (Brown University / Google AI)

## Recordings

### Day 1: Why Compositionality Matters for AI

<iframe width="560" height="315" src="https://www.youtube.com/embed/9tH9Qz1nH3k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- Gary Marcus -- "Compositionality and Natural Language Understanding"
- Allyson Ettinger -- "Shades of Meaning Composition: Defining Compositionality Goals in NLU" [[Slides](/pdfs/Ettinger.pdf)]
- Paul Smolensky -- "Human-Level Intelligence Requires Continuous, Robustly Compositional Representations: Neurocompositional Computing for NECST-Generation AI" [[Slides](/pdfs/Smolensky.pdf)]
- Raphaël Millière -- "Compositionality Without Constituency" [[Slides](/pdfs/Milliere.pdf)]

### Day 2: Can Language Models Handle Compositionality?

<iframe width="560" height="315" src="https://www.youtube.com/embed/GYB2dxvrvX0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- Dieuwke Hupkes -- "Are Neural Networks Compositional, and How Do We Even Know?"
- Tal Linzen -- "Successes and Failures of Compositionality in Neural Networks for Language" [[Slides](/pdfs/Linzen.pdf)]
- Stephanie Chan -- "Data Distributions Drive Emergent In-Context Learning in Transformers"
- Ellie Pavlick -- "No One Metric is Enough! Combining Evaluation Techniques to Uncover Latent Structure"
- Brenden Lake -- "Human-Like Compositional Generalization Through Meta-Learning"

## References

We have listed some relevant papers discussed by each speaker below. Note: this section will get gradually updated.

### Allyson Ettinger

[Slides](/pdfs/Ettinger.pdf)

- Pandia, L., Ettinger, A. (2021). Sorting through the noise: Testing robustness of information processing in pre-trained language models. Proceedings of The 2021 Conference on Empirical Methods in Natural Language Processing. [PDF](https://aclanthology.org/2021.emnlp-main.119.pdf)
- Yu, L., Ettinger, A. (2020). Assessing Phrasal Representation and Composition in Transformers. Proceedings of The 2020 Conference on Empirical Methods in Natural Language Processing. [PDF](https://aclanthology.org/2020.emnlp-main.397.pdf)
- Ettinger, A., Elgohary, A., Phillips, C., Resnik, P. (2018). Assessing Composition in Sentence Vector Representations. Proceedings of the 27th International Conference on Computational Linguistics. [PDF](https://aclanthology.org/C18-1152.pdf)

### Paul Smolensky

[Slides](/pdfs/Smolensky.pdf)

- Paul Smolensky, R. Thomas McCoy, Roland Fernandez, Matthew Goldrick, Jianfeng Gao. In press. Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems. AI Magazine. [PDF](http://arxiv.org/abs/2205.01128)
- Paul Smolensky, R. Thomas McCoy, Roland Fernandez, Matthew Goldrick, Jianfeng Gao. 2022. Neurocompositional computing in human and machine intelligence: A tutorial. Microsoft Technical Report MSR-TR-2022-5. [PDF](https://www.microsoft.com/en-us/research/publication/neurocompositional-computing-in-human-and-machine-intelligence-a-tutorial/)
- R. Thomas McCoy, Tal Linzen, Ewan Dunbar, Paul Smolensky. RNNs Implicitly Implement Tensor Product Representations. [PDF](https://arxiv.org/abs/1812.08718)
- Paul Soulos, Tom McCoy, Tal Linzen, Paul Smolensky. Discovering the Compositional Structure of Vector Representations with Role Learning Networks. [PDF](https://arxiv.org/abs/1910.09113)
- Paul Smolensky. On the proper treatment of connectionism. 1988. The Behavioral & Brain Sciences 11:1, 1–74. [PDF](/pdfs/Smolensky_BBS.pdf)
- March 1988 debate at MIT: [Fodor & Pylyshyn vs. Paul Smolensky](https://www.youtube.com/watch?v=EdWZpuXB5eA)

### Raphaël Millière

[Slides](/pdfs/Milliere.pdf)

- Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., … Wu, Z. (2022). Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (arXiv:2206.04615). [PDF](https://doi.org/10.48550/arXiv.2206.04615)
- Ontanon, S., Ainslie, J., Fisher, Z., & Cvicek, V. (2022). Making Transformers Solve Compositional Tasks. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 3591–3607. [PDF](https://aclanthology.org/2022.acl-long.251.pdf)
- Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., … Olah, C. (2021). A mathematical framework for transformer circuits. Transformer Circuits Thread. [Online](https://transformer-circuits.pub/2021/framework/index.html)
- Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., … Olah, C. (2022). In-context learning and induction heads. Transformer Circuits Thread. [Online](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)

### Tal Lizen

[Slides](/pdfs/Linzen.pdf)

- Najoung Kim & Tal Linzen (2020). COGS: A compositional generalization challenge based on semantic interpretation. EMNLP. [PDF](https://aclanthology.org/2020.emnlp-main.731/)
- Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, Marco Baroni (2018). Colorless green recurrent networks dream hierarchically. In Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1195–1205. [PDF](https://aclanthology.org/N18-1108/)
- R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao & Asli Celikyilmaz. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. [PDF](https://arxiv.org/abs/2111.09509)
- Linlu Qiu, Peter Shaw, Panupong Pasupat, Paweł Krzysztof Nowak, Tal Linzen, Fei Sha, Kristina Toutanova. Improving Compositional Generalization with Latent Structure and Data Augmentation. NAACL. [PDF](https://arxiv.org/abs/2112.07610)
- R. Thomas McCoy, Ellie Pavlick & Tal Linzen (2019). Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. ACL. [PDF](https://aclanthology.org/P19-1334/)